{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ed5gZ9F7_UuX",
        "outputId": "1d3df75b-ada8-4cd3-9839-aebe98197109"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/ai-201-b-mse-2-aiml-a/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1354899887.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mTARGET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NObeyesdad\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ai-201-b-mse-2-aiml-a/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtest\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ai-201-b-mse-2-aiml-a/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ai-201-b-mse-2-aiml-a/train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# ================================\n",
        "# CONFIG & LOAD DATA\n",
        "# ================================\n",
        "TARGET = \"NObeyesdad\"\n",
        "\n",
        "train = pd.read_csv(\"/kaggle/input/ai-201-b-mse-2-aiml-a/train.csv\")\n",
        "test  = pd.read_csv(\"/kaggle/input/ai-201-b-mse-2-aiml-a/test.csv\")\n",
        "\n",
        "train_df = train.copy()\n",
        "test_df  = test.copy()\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "# Drop ID from test BEFORE processing\n",
        "test_id = test_df[\"id\"]\n",
        "test_df = test_df.drop(columns=[\"id\"])\n",
        "# üîç EDA PART 1: NULL VALUE TABLE + BAR PLOT\n",
        "# ==========================================\n",
        "print(\"\\nChecking null values...\\n\")\n",
        "nulls = train_df.isnull().sum()\n",
        "\n",
        "print(nulls)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.barplot(x=nulls.index, y=nulls.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Null Values in Each Column\")\n",
        "plt.show()\n",
        "# ================================\n",
        "# 1Ô∏è‚É£ Split X & y\n",
        "# ================================\n",
        "X = train_df.drop(columns=[TARGET])\n",
        "y = train_df[TARGET]\n",
        "\n",
        "# ================================\n",
        "# 2Ô∏è‚É£ Numeric & Categorical Columns\n",
        "# ================================\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Numeric columns:\", len(num_cols))\n",
        "print(\"Categorical columns:\", len(cat_cols))\n",
        "# ==========================================\n",
        "# üîç EDA PART 2: NUMERICAL COLUMN DISTRIBUTIONS\n",
        "# ==========================================\n",
        "print(\"\\nPlotting numeric distributions...\\n\")\n",
        "X[num_cols].hist(bins=30, figsize=(12,10))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# üîç EDA PART 3: CATEGORY-WISE DISTRIBUTION (COUNT PLOTS)\n",
        "# ==========================================\n",
        "print(\"\\nPlotting category-wise countplots...\\n\")\n",
        "for col in cat_cols:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.countplot(data=X, x=col)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(f\"Countplot of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "# ================================\n",
        "# 3Ô∏è‚É£ Missing value imputation\n",
        "# ================================\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "test_df[num_cols] = num_imputer.transform(test_df[num_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "X[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\n",
        "test_df[cat_cols] = cat_imputer.transform(test_df[cat_cols])\n",
        "\n",
        "# üîç EDA PART 4: OUTLIER BOXPLOTS FOR NUMERIC COLUMNS\n",
        "# ==========================================\n",
        "print(\"\\nPlotting numeric outliers (boxplots)...\\n\")\n",
        "for col in num_cols:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.boxplot(x=X[col])\n",
        "    plt.title(f\"Outlier Boxplot for {col}\")\n",
        "    plt.show()\n",
        "# ================================\n",
        "# 4Ô∏è‚É£ Outlier Capping Category-wise\n",
        "# ================================\n",
        "def cap_iqr_categorywise(df, cat_cols, num_cols):\n",
        "    df = df.copy()\n",
        "\n",
        "    def cap_group(group):\n",
        "        group = group.copy()\n",
        "        for col in num_cols:\n",
        "            Q1 = group[col].quantile(0.25)\n",
        "            Q3 = group[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            low = Q1 - 1.5 * IQR\n",
        "            high = Q3 + 1.5 * IQR\n",
        "            group[col] = group[col].clip(low, high)\n",
        "        return group\n",
        "\n",
        "    for c in cat_cols:\n",
        "        df = df.groupby(c, group_keys=False, observed=True).apply(cap_group)\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Capping Outliers...\")\n",
        "X = cap_iqr_categorywise(X, cat_cols, num_cols)\n",
        "\n",
        "# ================================\n",
        "# 5Ô∏è‚É£ One-Hot Encoding\n",
        "# ================================\n",
        "print(\"Encoding categorical columns...\")\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "X_cat = ohe.fit_transform(X[cat_cols])\n",
        "test_cat = ohe.transform(test_df[cat_cols])\n",
        "\n",
        "X_cat_df = pd.DataFrame(X_cat, index=X.index, columns=ohe.get_feature_names_out(cat_cols))\n",
        "test_cat_df = pd.DataFrame(test_cat, index=test_df.index, columns=ohe.get_feature_names_out(cat_cols))\n",
        "\n",
        "X = pd.concat([X.drop(columns=cat_cols), X_cat_df], axis=1)\n",
        "test_df = pd.concat([test_df.drop(columns=cat_cols), test_cat_df], axis=1)\n",
        "\n",
        "# ================================\n",
        "# 6Ô∏è‚É£ Standard Scaler\n",
        "# ================================\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
        "\n",
        "# ================================\n",
        "# 7Ô∏è‚É£ Train/Valid split\n",
        "# ================================\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 8Ô∏è‚É£ RandomForest + RandomSearch\n",
        "# ================================\n",
        "rf = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "\n",
        "param_dist = {\n",
        "    \"n_estimators\": [200, 400, 600, 800],\n",
        "    \"max_depth\": [None, 10, 20, 25],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"sqrt\", \"log2\"]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=25,\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining RandomForest...\")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "print(\"\\nBest Parameters:\", search.best_params_)\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ================================\n",
        "# 9Ô∏è‚É£ Validation Metrics\n",
        "# ================================\n",
        "pred_valid = best_model.predict(X_valid)\n",
        "# ==========================================\n",
        "# üîµ CONFUSION MATRIX\n",
        "# ==========================================\n",
        "cm = confusion_matrix(y_valid, pred_valid)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# üîµ CLASSIFICATION REPORT HEATMAP\n",
        "# ==========================================\n",
        "report = classification_report(y_valid, pred_valid, output_dict=True)\n",
        "report_df = pd.DataFrame(report).T\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap=\"Greens\")\n",
        "plt.title(\"Classification Report Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# üîµ MULTICLASS ROC CURVE (One-vs-Rest)\n",
        "# ==========================================\n",
        "\n",
        "y_valid_bin = label_binarize(y_valid, classes=best_model.classes_)\n",
        "valid_probs = best_model.predict_proba(X_valid)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "for i, cls in enumerate(best_model.classes_):\n",
        "    fpr, tpr, _ = roc_curve(y_valid_bin[:, i], valid_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{cls} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0,1], [0,1], 'k--')  # diagonal line\n",
        "plt.title(\"Multiclass ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_valid, pred_valid))\n",
        "print(\"Macro F1:\", f1_score(y_valid, pred_valid, average='macro'))\n",
        "print(classification_report(y_valid, pred_valid))\n",
        "\n",
        "# ==================================================\n",
        "# üîü FIX: Make sure test_df columns match training\n",
        "# ==================================================\n",
        "test_df = test_df[X_train.columns]\n",
        "\n",
        "# ================================\n",
        "# üîü Predict on Test\n",
        "# ================================\n",
        "test_probs = best_model.predict_proba(test_df)\n",
        "classes = best_model.classes_\n",
        "\n",
        "submission = pd.DataFrame(test_probs, columns=[f\"{TARGET}_{c}\" for c in classes])\n",
        "submission.insert(0, \"id\", test_id)\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\nsubmission.csv saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class labels\n",
        "test_pred = best_model.predict(test_df)\n",
        "\n",
        "# Build submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_id,\n",
        "    TARGET: test_pred\n",
        "})\n",
        "\n",
        "submission.to_csv(\"verma.csv\", index=False)\n",
        "print(\"Submission saved!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# test_probs = best_model.predict_proba(test_df)\n",
        "# classes = best_model.classes_\n",
        "\n",
        "# submission = pd.DataFrame(test_probs, columns=[f\"{TARGET}_{c}\" for c in classes])\n",
        "# submission.insert(0, \"id\", test_id)\n",
        "\n",
        "# submission.to_csv(\"submission.csv\", index=False)\n",
        "# print(\"\\nsubmission.csv saved!\")"
      ],
      "metadata": {
        "id": "QEiRBziB_bOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnrCHET1_b6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}